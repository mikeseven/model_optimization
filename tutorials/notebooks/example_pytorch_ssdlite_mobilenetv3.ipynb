{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c261298-309f-41e8-9338-a5e205f09b05",
   "metadata": {},
   "source": [
    "# Post Training Quantization a Pytorch Object Detection Model - A Quick-Start Guide\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/example_pytorch_ssdlite_mobilenetv3.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial shows how to quantize a pre-trained object detection model from the torchvision package using the Model-Compression-Toolkit (MCT). We will do so by giving an example of MCT's post-training quantization. As we will see, post-training quantization is a low complexity yet effective quantization method. In this example, we will quantize the model and evaluate the accuracy before and after quantization.\n",
    "\n",
    "As the pretrained object detection model contains a preprocessing and postprocessing layers that their quantization with MCT is out of this notebook's scope, we'll separate these layers from the model-to-quantize. These layers will be included in the evaluation code.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial we will cover:\n",
    "\n",
    "1. Post-Training Quantization using MCT.\n",
    "2. Loading and preprocessing COCO's validation dataset.\n",
    "3. Loading and preprocessing an unlabeled representative dataset from the COCO trainset.\n",
    "4. Accuracy evaluation of the floating-point and the quantized models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ce67a-ce08-4f5a-bf70-e54c63774163",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install and import the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bc130-3ed1-4815-8fd9-520fa66db8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q pycocotools\n",
    "!pip install -q model-compression-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed80e16-1579-4274-9f3b-3939da8dd8a2",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.ssdlite import SSDLite320_MobileNet_V3_Large_Weights\n",
    "from torchvision.models.detection.anchor_utils import ImageList\n",
    "import model_compression_toolkit as mct\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c2b8b-3175-4d46-a18a-7c4d8b6fcb38",
   "metadata": {},
   "source": [
    "## Float Model\n",
    "\n",
    "### Load float model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8395b28-4732-4d18-b081-5d3bdf508691",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "image_size = (320, 320)\n",
    "model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT)\n",
    "# mAP=0.2131 (float)\n",
    "# mAP=0.2007 (quantized)\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f6df2-f812-4fd5-91e6-db5d12d96713",
   "metadata": {},
   "source": [
    "### Evaluate float model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a1499-5b24-4737-969d-0c27dca97ea5",
   "metadata": {},
   "source": [
    "#### Create the COCO evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a0fc0e-ec2f-465a-987d-c7a4d632296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(outputs, img_ids):\n",
    "    detections = []\n",
    "\n",
    "    # Process model outputs and convert to detection format\n",
    "    for idx, output in enumerate(outputs):\n",
    "        image_id = img_ids[idx]  # Adjust according to your batch size and indexing\n",
    "        scores = output['scores'].cpu().numpy()\n",
    "        labels = output['labels'].cpu().numpy()\n",
    "        boxes = output['boxes'].cpu().numpy()\n",
    "\n",
    "        for score, label, box in zip(scores, labels, boxes):\n",
    "            detection = {\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": label,\n",
    "                \"bbox\": [box[0], box[1], box[2] - box[0], box[3] - box[1]],\n",
    "                \"score\": score\n",
    "            }\n",
    "            detections.append(detection)\n",
    "\n",
    "    return detections\n",
    "\n",
    "\n",
    "class CocoEval:\n",
    "    def __init__(self, path2json):\n",
    "\n",
    "        # Load ground truth annotations\n",
    "        self.coco_gt = COCO(path2json)\n",
    "\n",
    "        # A list of reformatted model outputs\n",
    "        self.all_detections = []\n",
    "\n",
    "    def add_batch_detections(self, outputs, targets):\n",
    "\n",
    "        # Collect and format results from the batch\n",
    "        img_ids, _outs = [], []\n",
    "        for t, o in zip(targets, outputs):\n",
    "            if len(t) > 0:\n",
    "                img_ids.append(t[0]['image_id'])\n",
    "                _outs.append(o)\n",
    "\n",
    "        batch_detections = format_results(_outs, img_ids)  # Implement this function\n",
    "\n",
    "        self.all_detections.extend(batch_detections)\n",
    "\n",
    "    def result(self):\n",
    "        # Initialize COCO evaluation object\n",
    "        self.coco_dt = self.coco_gt.loadRes(self.all_detections)\n",
    "        coco_eval = COCOeval(self.coco_gt, self.coco_dt, 'bbox')\n",
    "\n",
    "        # Run evaluation\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "        # Print mAP results\n",
    "        print(\"mAP: {:.4f}\".format(coco_eval.stats[0]))\n",
    "\n",
    "        return coco_eval.stats\n",
    "\n",
    "    def reset(self):\n",
    "        self.all_detections = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde2f8e-0642-4374-a1f4-df2775fe7767",
   "metadata": {},
   "source": [
    "#### Evaluate float model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56393342-cecf-4f64-b9ca-2f515c765942",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DATASET_FOLDER = '/path/to/coco/evaluation/images/val2017'\n",
    "EVAL_DATASET_ANNOTATION_FILE = '/path/to/coco/annotations/instances_val2017.json'\n",
    "\n",
    "\n",
    "# The float model accepts a list of images in their original shapes and preprocesses them inside, so collate the batch images as a list\n",
    "def collate_fn(batch_input):\n",
    "    images = [b[0] for b in batch_input]\n",
    "    targets = [b[1] for b in batch_input]\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "# Initialize the COCO evaluation DataLoader\n",
    "coco_eval = torchvision.datasets.CocoDetection(root=EVAL_DATASET_FOLDER,\n",
    "                                               annFile=EVAL_DATASET_ANNOTATION_FILE,\n",
    "                                               transform=torchvision.transforms.ToTensor())\n",
    "batch_size = 50\n",
    "data_loader = torch.utils.data.DataLoader(coco_eval, batch_size=batch_size, shuffle=False,\n",
    "                                          num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize the evaluation metric object\n",
    "coco_metric = CocoEval(EVAL_DATASET_ANNOTATION_FILE)\n",
    "\n",
    "# Iterate and evaluate the COCO evaluation set\n",
    "for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "    # Run inference on the batch\n",
    "    images = list(image.to(device) for image in images)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    # Add the model outputs to metric object (a dictionary of outputs after postprocess: boxes, scores & classes)\n",
    "    coco_metric.add_batch_detections(outputs, targets)\n",
    "    if (batch_idx+1) % 10 == 0:\n",
    "        print(f'processed {(batch_idx+1)*data_loader.batch_size} images')\n",
    "\n",
    "# Print float model mAP results\n",
    "print(\"Float model mAP: {:.4f}\".format(coco_metric.result()[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015e760b-6555-45b4-aaf9-500e974c1d86",
   "metadata": {},
   "source": [
    "## Quantize Model\n",
    "\n",
    "### Extract model to be quantized\n",
    "\n",
    "Extract the float model's backcone and head, and construct a torch model that only contains them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e90967-594b-480f-b2e6-45e2c9ce9cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDD4Quant(torch.nn.Module):\n",
    "    def __init__(self, in_sdd, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Save the float model under self.base as a module of the model. Later we'll only run \"backbone\" & \"head\"\n",
    "        self.add_module(\"base\", in_sdd)\n",
    "\n",
    "    # Forward pass of the model to be quantized. This code is copied from the float model forward function (removed the preprocess and postprocess code)\n",
    "    def forward(self, x):\n",
    "        features = self.base.backbone(x)\n",
    "\n",
    "        features = list(features.values())\n",
    "\n",
    "        # compute the ssd heads outputs using the features\n",
    "        head_outputs = self.base.head(features)\n",
    "        return head_outputs\n",
    "\n",
    "\n",
    "model4quant = SDD4Quant(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb59fd-3877-45b4-8529-7f9edb687c69",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract preproecss and postprocess\n",
    "\n",
    "Extract the preprocess and postprocess functions from the float model object, and construct separate preprocess and postprocess functions for the representative dataset and evaluation code\n",
    "\n",
    "\n",
    "Note: the MCT output model flattens the float model output data structure to a list, so the PostProcess manually rebuilds it as the original data structure (a dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff336c30-56c9-4de8-9c42-6462ddb8d2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(image, targets):\n",
    "    # need to save the original image sizes before resize for the postprocess part\n",
    "    targets = {'gt': targets, 'img_size': list(image.size[::-1])}\n",
    "    image = model.transform([torchvision.transforms.ToTensor()(image)])[0].tensors[0, ...]\n",
    "    return image, targets\n",
    "\n",
    "\n",
    "# Define the postprocess, which is the code copied from the float model forward code. These layers will not be quantized.\n",
    "class PostProcess:\n",
    "    def __init__(self):\n",
    "        self.features = [torch.zeros((1, 1, s, s)) for s in [20, 10, 5, 3, 2, 1]]\n",
    "\n",
    "    def __call__(self, head_outputs, image_list, original_image_sizes):\n",
    "        anchors = [a.to(device) for a in model.anchor_generator(image_list, self.features)]\n",
    "\n",
    "        # The MCT flattens the outputs of the head to a list, so need to change it to a dictionary as the psotprocess functions expect.\n",
    "        if not isinstance(head_outputs, dict):\n",
    "            if head_outputs[0].shape[-1] == 4:\n",
    "                head_outputs = {\"bbox_regression\": head_outputs[0],\n",
    "                                \"cls_logits\": head_outputs[1]}\n",
    "            else:\n",
    "                head_outputs = {\"bbox_regression\": head_outputs[1],\n",
    "                                \"cls_logits\": head_outputs[0]}\n",
    "\n",
    "        # Float model postprocess functions that handle box regression and NMS\n",
    "        detections = model.postprocess_detections(head_outputs, anchors, image_list.image_sizes)\n",
    "        detections = model.transform.postprocess(detections, image_list.image_sizes, original_image_sizes)\n",
    "        return detections\n",
    "\n",
    "\n",
    "postprocess = PostProcess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef70dd-d513-48ae-b7d0-4e1cac164d06",
   "metadata": {},
   "source": [
    "### Dataset preparation\n",
    "\n",
    "Assuming we've downloaded the COCO dataset to a folder, let's set the folder path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5d8af-7cbd-4b1c-9b20-aac316b7bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_FOLDER = '/path/to/coco/training/images/train2017'\n",
    "TRAIN_DATASET_ANNOTATION_FILE = '/path/to/coco/annotations/instances_train2017.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80ebaf-7ae6-4b34-ae48-8463fc47a40d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, let's create two dataset loader objects:\n",
    "* Train DataLoader that we'll use to create the representative dataset for the quantization calibration.\n",
    "* Evaluation DataLoader that we'll use the evaluate the quantized model.\n",
    "\n",
    "Note that both objects include the \"preprocess\" function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92968e1-cd96-44a1-9ced-bcefad721de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(batch_input):\n",
    "    # collating images for the quantized model should return a single tensor: [B, C, H, W]\n",
    "    images = torch.stack([b[0] for b in batch_input])\n",
    "    targets = [b[1] for b in batch_input]\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "coco_train = torchvision.datasets.CocoDetection(root=TRAIN_DATASET_FOLDER, annFile=TRAIN_DATASET_ANNOTATION_FILE,\n",
    "                                                transforms=preprocess)\n",
    "train_loader = torch.utils.data.DataLoader(coco_train, batch_size=16, shuffle=False, num_workers=0,\n",
    "                                           collate_fn=train_collate_fn)\n",
    "\n",
    "coco_eval = torchvision.datasets.CocoDetection(root=EVAL_DATASET_FOLDER, annFile=EVAL_DATASET_ANNOTATION_FILE,\n",
    "                                               transforms=preprocess)\n",
    "eval_loader = torch.utils.data.DataLoader(coco_eval, batch_size=50, shuffle=False, num_workers=0,\n",
    "                                          collate_fn=train_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d769fc-0c8f-40ce-8a97-2f69a224d73f",
   "metadata": {},
   "source": [
    "### Quantize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f39855-c63b-4e0f-844f-317f9ec8a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representative_dataset(n_iter):\n",
    "    \n",
    "    def representative_dataset():\n",
    "        ds_iter = iter(train_loader)\n",
    "        for _ in range(n_iter):\n",
    "            yield [next(ds_iter)[0]]\n",
    "\n",
    "    return representative_dataset\n",
    "\n",
    "\n",
    "quant_model, _ = mct.ptq.pytorch_post_training_quantization_experimental(model4quant,\n",
    "                                                                         get_representative_dataset(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6bffc-23d1-4852-8ec5-9007361c8eeb",
   "metadata": {},
   "source": [
    "### Evaluate quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7b87c-a9f4-4568-885a-fe009c8f4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_metric = CocoEval(EVAL_DATASET_ANNOTATION_FILE)\n",
    "for batch_idx, (images, targets) in enumerate(eval_loader):\n",
    "    # Run inference on the batch\n",
    "    with torch.no_grad():\n",
    "        outputs = quant_model(images.to(device))\n",
    "    \n",
    "    image_hw = [t['img_size'] for t in targets]\n",
    "    image_list = ImageList(images, [image_size] * images.shape[0])\n",
    "    detections = postprocess(outputs, image_list, image_hw)\n",
    "\n",
    "    coco_metric.add_batch_detections(detections, [t['gt'] for t in targets])\n",
    "    if (batch_idx+1) % 10 == 0:\n",
    "        print(f'processed {(batch_idx+1)*data_loader.batch_size} images')\n",
    "\n",
    "# Print mAP results\n",
    "print(\"Quantized model mAP: {:.4f}\".format(coco_metric.result()[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
